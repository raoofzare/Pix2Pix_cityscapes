{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1568850,
          "sourceType": "datasetVersion",
          "datasetId": 927102
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import torchvision.utils as vutils"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:45:54.545569Z",
          "iopub.execute_input": "2024-12-12T17:45:54.546366Z",
          "iopub.status.idle": "2024-12-12T17:45:58.880269Z",
          "shell.execute_reply.started": "2024-12-12T17:45:54.546330Z",
          "shell.execute_reply": "2024-12-12T17:45:58.879317Z"
        },
        "id": "-oH1RH0yDKzX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset, and DataLoader"
      ],
      "metadata": {
        "id": "7rhOBDCFDKzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom Dataset for Cityscapes\n",
        "class CityscapesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, directory, transform=None):\n",
        "        self.directory = directory\n",
        "        self.image_files = sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.png', '.jpg'))])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the concatenated image as a PIL image\n",
        "        image = Image.open(self.image_files[idx]).convert(\"RGB\")\n",
        "        width, height = image.size\n",
        "\n",
        "        # Split into input and output\n",
        "        input = image.crop((width // 2, 0, width, height))  # Right half (input)\n",
        "        output = image.crop((0, 0, width // 2, height))  # Left half (output)\n",
        "\n",
        "        # Apply paired transformations if provided\n",
        "        if self.transform:\n",
        "          input, output= self.transform([input, output])\n",
        "\n",
        "        return input, output\n",
        "\n",
        "\n",
        "# Custom transform class to apply the same transformation to both input and output\n",
        "class PairedResize(torch.nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.resize = transforms.Resize(size)\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "        return [self.resize(img) for img in imgs]\n",
        "\n",
        "\n",
        "class PairedRandomCrop(torch.nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "        width, height = imgs[0].size\n",
        "        crop_width, crop_height = self.size\n",
        "\n",
        "        top = random.randint(0, height - crop_height)\n",
        "        left = random.randint(0, width - crop_width)\n",
        "\n",
        "        crops = [img.crop((left, top, left + crop_width, top + crop_height)) for img in imgs]\n",
        "        return crops\n",
        "\n",
        "\n",
        "class PairedRandomHorizontalFlip(torch.nn.Module):\n",
        "    def __init__(self, flip_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.flip_prob = flip_prob\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "        if random.random() < self.flip_prob:\n",
        "          return [transforms.functional.hflip(img) for img in imgs]\n",
        "        return imgs\n",
        "\n",
        "\n",
        "class PairedToTensor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "        return [transforms.ToTensor()(img) for img in imgs]\n",
        "\n",
        "\n",
        "# Define paired transformations\n",
        "paired_transform_train = transforms.Compose([\n",
        "    PairedResize((286, 286)),\n",
        "    PairedRandomCrop((256, 256)),\n",
        "    PairedRandomHorizontalFlip(),\n",
        "    PairedToTensor(),\n",
        "])\n",
        "\n",
        "paired_transform_test = transforms.Compose([\n",
        "    PairedToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets for train and validation\n",
        "train_dir = \"/kaggle/input/cityscapes-pix2pix-dataset/train\"\n",
        "val_dir = \"/kaggle/input/cityscapes-pix2pix-dataset/val\"\n",
        "\n",
        "train_dataset = CityscapesDataset(train_dir, transform=paired_transform_train)\n",
        "val_dataset = CityscapesDataset(val_dir, transform=paired_transform_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:45:58.882039Z",
          "iopub.execute_input": "2024-12-12T17:45:58.882661Z",
          "iopub.status.idle": "2024-12-12T17:45:58.936359Z",
          "shell.execute_reply.started": "2024-12-12T17:45:58.882614Z",
          "shell.execute_reply": "2024-12-12T17:45:58.935560Z"
        },
        "id": "4tcWDyIZDKzd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
        "\n",
        "# Iterate through the train DataLoader\n",
        "for index, (inputs, outputs) in enumerate(train_loader):\n",
        "    if index < 3:  # Display three pairs of images\n",
        "        input_img = inputs.squeeze(0).permute(1, 2, 0).numpy()  # Convert to HWC\n",
        "        output_img = outputs.squeeze(0).permute(1, 2, 0).numpy()\n",
        "\n",
        "        axes[index, 0].imshow(input_img)\n",
        "        axes[index, 0].set_title(f\"Input Image (Train Set) {index+1}\")\n",
        "        axes[index, 0].axis('off')\n",
        "\n",
        "        axes[index, 1].imshow(output_img)\n",
        "        axes[index, 1].set_title(f\"Output Image (Train Set) {index+1}\")\n",
        "        axes[index, 1].axis('off')\n",
        "    else:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:45:58.937459Z",
          "iopub.execute_input": "2024-12-12T17:45:58.937783Z",
          "iopub.status.idle": "2024-12-12T17:45:59.725879Z",
          "shell.execute_reply.started": "2024-12-12T17:45:58.937743Z",
          "shell.execute_reply": "2024-12-12T17:45:59.724803Z"
        },
        "id": "vw_YKlhCDKze"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
        "\n",
        "# Iterate through the train DataLoader\n",
        "for index, (inputs, outputs) in enumerate(val_loader):\n",
        "    if index < 3:  # Display three pairs of images\n",
        "        input_img = inputs.squeeze(0).permute(1, 2, 0).numpy()  # Convert to HWC\n",
        "        output_img = outputs.squeeze(0).permute(1, 2, 0).numpy()\n",
        "\n",
        "        axes[index, 0].imshow(input_img)\n",
        "        axes[index, 0].set_title(f\"Input Image (Test Set) {index+1}\")\n",
        "        axes[index, 0].axis('off')\n",
        "\n",
        "        axes[index, 1].imshow(output_img)\n",
        "        axes[index, 1].set_title(f\"Output Image (Test Set) {index+1}\")\n",
        "        axes[index, 1].axis('off')\n",
        "    else:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:45:59.728012Z",
          "iopub.execute_input": "2024-12-12T17:45:59.728366Z",
          "iopub.status.idle": "2024-12-12T17:46:00.543589Z",
          "shell.execute_reply.started": "2024-12-12T17:45:59.728328Z",
          "shell.execute_reply": "2024-12-12T17:46:00.542714Z"
        },
        "id": "nwdCh3GwDKzg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvBlock Class\n",
        "\n",
        "This class defines a reusable building block for convolutional layers.\n",
        "* Initializes with `in_channels`, `out_channels`, `apply_bn`, and `apply_leaky` flags.\n",
        "* Creates a 2D convolutional layer with kernel size 4, stride 2, padding 1, and no bias.\n",
        "* Optionally adds a BatchNorm2d layer.\n",
        "* Sets the activation function to LeakyReLU or ReLU.\n",
        "* In the forward pass, it applies convolution, optional batch normalization, and the activation function.\n",
        "\n",
        "# UpSample Class\n",
        "\n",
        "This class defines a building block for upsampling operations in the decoder.\n",
        "* Initializes with `in_channels`, `out_channels`, and `apply_dropout` flags.\n",
        "* Creates a sequence of layers:\n",
        "    * Transposed convolutional layer to upsample the feature map.\n",
        "    * BatchNorm2d layer.\n",
        "    * ReLU activation.\n",
        "    * Optional Dropout layer.\n",
        "* In the forward pass, it applies the sequence of layers to the input.\n",
        "\n",
        "# PatchDiscriminator Class\n",
        "\n",
        "This class implements a patch-based discriminator network for image generation.\n",
        "* Initializes with the number of input channels.\n",
        "* Creates a sequence of layers using `nn.Sequential`:\n",
        "    * First ConvBlock with 64 channels and no batch normalization.\n",
        "    * Subsequent ConvBlocks with increasing channels and batch normalization.\n",
        "    * Convolutional layers with 512 and 1 channels.\n",
        "    * Sigmoid activation for output.\n",
        "* In the forward pass, concatenates input and generated images, and passes through the layers.\n",
        "\n",
        "# UnetGenerator Class\n",
        "\n",
        "This class implements a U-Net generator architecture for image-to-image translation.\n",
        "* Initializes with input and output channels.\n",
        "* Creates encoder blocks (`en1` to `en8`) using ConvBlock.\n",
        "* Creates decoder blocks (`de1` to `de7`) using custom `UpSample` class.\n",
        "* Creates a final convolutional transpose layer (`de8`) and Tanh activation.\n",
        "* In the forward pass,\n",
        "    * Passes input through encoder,\n",
        "    * Passes encoder output through decoder with upsampling and concatenation,\n",
        "    * Applies final convolutional transpose and Tanh activation."
      ],
      "metadata": {
        "id": "xKEmwrM7DKzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional block with optional batch normalization and activation.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input channels.\n",
        "        out_channels (int): Number of output channels.\n",
        "        apply_bn (bool, optional): Whether to apply batch normalization. Defaults to True.\n",
        "        apply_leaky (bool, optional): Whether to use LeakyReLU activation. Defaults to True.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, apply_bn=True, apply_leaky=True):\n",
        "        super().__init__()\n",
        "        self.apply_bn = apply_bn\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        if apply_bn:\n",
        "            self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.LeakyReLU(0.2, inplace=True) if apply_leaky else nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.apply_bn and x.size(2) > 1 and x.size(3) > 1:  # Only apply BatchNorm for size > 1\n",
        "            x = self.bn(x)\n",
        "        return self.activation(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:46:00.544758Z",
          "iopub.execute_input": "2024-12-12T17:46:00.545082Z",
          "iopub.status.idle": "2024-12-12T17:46:00.553641Z",
          "shell.execute_reply.started": "2024-12-12T17:46:00.545048Z",
          "shell.execute_reply": "2024-12-12T17:46:00.552775Z"
        },
        "id": "dD0Z-yMsDKzh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling block with transposed convolution, batch normalization, and optional dropout.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input channels.\n",
        "        out_channels (int): Number of output channels.\n",
        "        apply_dropout (bool, optional): Whether to apply dropout. Defaults to False.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, apply_dropout=False):\n",
        "        super().__init__()\n",
        "        layers = list()\n",
        "        layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "        if apply_dropout:\n",
        "            layers.append(nn.Dropout(0.5))\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:46:00.554839Z",
          "iopub.execute_input": "2024-12-12T17:46:00.555128Z",
          "iopub.status.idle": "2024-12-12T17:46:00.567469Z",
          "shell.execute_reply.started": "2024-12-12T17:46:00.555101Z",
          "shell.execute_reply": "2024-12-12T17:46:00.566555Z"
        },
        "id": "vdckA65CDKzi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchDiscriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch-based discriminator for image-to-image translation.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int, optional): Number of input channels. Defaults to 3.\n",
        "\n",
        "    Attributes:\n",
        "        layers: nn.Sequential containing the discriminator layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            ConvBlock(2*in_channels, 64, apply_bn=False),  # 64x128x128\n",
        "            ConvBlock(64, 128),  # 128x64x64\n",
        "            ConvBlock(128, 256),  # 256x32x32\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1, bias=False),  # 512x31x31\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),  # 1x30x30\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_image, generated_image):\n",
        "        \"\"\"\n",
        "        Forward pass of the discriminator.\n",
        "\n",
        "        Args:\n",
        "            input_image: Input image tensor.\n",
        "            generated_image: Generated image tensor.\n",
        "\n",
        "        Returns:\n",
        "            Discriminator output (probability score).\n",
        "        \"\"\"\n",
        "        return self.layers(torch.cat([input_image, generated_image], dim=1))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:46:00.568611Z",
          "iopub.execute_input": "2024-12-12T17:46:00.568884Z",
          "iopub.status.idle": "2024-12-12T17:46:00.579900Z",
          "shell.execute_reply.started": "2024-12-12T17:46:00.568858Z",
          "shell.execute_reply": "2024-12-12T17:46:00.579023Z"
        },
        "id": "GBVoHBQpDKzi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net Generator for image-to-image translation.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int, optional): Number of input channels. Defaults to 3.\n",
        "        out_channels (int, optional): Number of output channels. Defaults to 3.\n",
        "\n",
        "    Attributes:\n",
        "        en1-en8: Encoder blocks (ConvBlocks).\n",
        "        de1-de7: Decoder blocks (UpSample blocks).\n",
        "        de8: Final convolutional transpose layer.\n",
        "        final_activation: Tanh activation function.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super().__init__()\n",
        "        self.en1 = ConvBlock(in_channels, 64, apply_bn=False)  # 64x128x128\n",
        "        self.en2 = ConvBlock(64, 128)  # 128x64x64\n",
        "        self.en3 = ConvBlock(128, 256)  # 256x32x32\n",
        "        self.en4 = ConvBlock(256, 512)  # 512x16x16\n",
        "        self.en5 = ConvBlock(512, 512)  # 512x8x8\n",
        "        self.en6 = ConvBlock(512, 512)  # 512x4x4\n",
        "        self.en7 = ConvBlock(512, 512)  # 512x2x2\n",
        "        self.en8 = ConvBlock(512, 512)  # 512x1x1\n",
        "\n",
        "        self.de1 = UpSample(512, 512, apply_dropout=True)  # 512x2x2\n",
        "        self.de2 = UpSample(1024, 512, apply_dropout=True)  # 512x4x4\n",
        "        self.de3 = UpSample(1024, 512, apply_dropout=True)  # 512x8x8\n",
        "        self.de4 = UpSample(1024, 512)  # 512x16x16\n",
        "        self.de5 = UpSample(1024, 256)  # 256x32x32\n",
        "        self.de6 = UpSample(512, 128)  # 128x64x64\n",
        "        self.de7 = UpSample(256, 64)  # 64x128x128\n",
        "\n",
        "        self.de8 = nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1)  # 3x256x256\n",
        "        self.final_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the U-Net generator.\n",
        "\n",
        "        Args:\n",
        "            x: Input image tensor (e.g., 3x256x256).\n",
        "\n",
        "        Returns:\n",
        "            Generated output image tensor (e.g., 3x256x256).\n",
        "        \"\"\"\n",
        "        e1 = self.en1(x)  # 64x128x128\n",
        "        e2 = self.en2(e1)  # 128x64x64\n",
        "        e3 = self.en3(e2)  # 256x32x32\n",
        "        e4 = self.en4(e3)  # 512x16x16\n",
        "        e5 = self.en5(e4)  # 512x8x8\n",
        "        e6 = self.en6(e5)  # 512x4x4\n",
        "        e7 = self.en7(e6)  # 512x2x2\n",
        "        e8 = self.en8(e7)  # 512x1x1\n",
        "\n",
        "        d1 = self.de1(e8)  # 512x2x2\n",
        "        d2 = self.de2(torch.cat([d1, e7], dim=1))  # 512x4x4\n",
        "        d3 = self.de3(torch.cat([d2, e6], dim=1))  # 512x8x8\n",
        "        d4 = self.de4(torch.cat([d3, e5], dim=1))  # 512x16x16\n",
        "        d5 = self.de5(torch.cat([d4, e4], dim=1))  # 256x32x32\n",
        "        d6 = self.de6(torch.cat([d5, e3], dim=1))  # 128x64x64\n",
        "        d7 = self.de7(torch.cat([d6, e2], dim=1))  # 64x128x128\n",
        "\n",
        "        d8 = self.de8(torch.cat([d7, e1], dim=1))  # 3x256x256\n",
        "        return self.final_activation(d8)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:46:00.581878Z",
          "iopub.execute_input": "2024-12-12T17:46:00.582735Z",
          "iopub.status.idle": "2024-12-12T17:46:00.594594Z",
          "shell.execute_reply.started": "2024-12-12T17:46:00.582696Z",
          "shell.execute_reply": "2024-12-12T17:46:00.593719Z"
        },
        "id": "pSY7NGXdDKzj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "59ceiKDxDKzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "lr = 2e-4\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "num_epochs = 50\n",
        "lambda_l1 = 100  # Weight for L1 loss\n",
        "checkpoint_path = \"/kaggle/working/pix2pix_checkpoint.pth\"\n",
        "\n",
        "# Initialize Generator and Discriminator\n",
        "generator = UnetGenerator().cuda()\n",
        "discriminator = PatchDiscriminator().cuda()\n",
        "\n",
        "# Define Loss Functions\n",
        "criterion_GAN = nn.BCEWithLogitsLoss()\n",
        "criterion_L1 = nn.L1Loss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "# Track losses\n",
        "generator_losses = []\n",
        "discriminator_losses = []\n",
        "start_epoch = 0\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
        "    discriminator.load_state_dict(checkpoint[\"discriminator_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G_state_dict\"])\n",
        "    optimizer_D.load_state_dict(checkpoint[\"optimizer_D_state_dict\"])\n",
        "    generator_losses = checkpoint[\"generator_losses\"]\n",
        "    discriminator_losses = checkpoint[\"discriminator_losses\"]\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}.\")\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    epoch_G_loss = 0.0\n",
        "    epoch_D_loss = 0.0\n",
        "\n",
        "    for batch_idx, (input_image, target_image) in enumerate(loop):\n",
        "        input_image = input_image.cuda()\n",
        "        target_image = target_image.cuda()\n",
        "        # print(input_image.shape)\n",
        "        # print(target_image.shape)\n",
        "        ### Train Discriminator ###\n",
        "        fake_image = generator(input_image)\n",
        "\n",
        "        # Discriminator on real images\n",
        "        D_real = discriminator(input_image, target_image)\n",
        "        D_real_loss = criterion_GAN(D_real, torch.ones_like(D_real).cuda())\n",
        "\n",
        "        # Discriminator on fake images\n",
        "        D_fake = discriminator(input_image, fake_image.detach())\n",
        "        D_fake_loss = criterion_GAN(D_fake, torch.zeros_like(D_fake).cuda())\n",
        "\n",
        "        # Total Discriminator loss\n",
        "        D_loss = (D_real_loss + D_fake_loss) / 2\n",
        "        optimizer_D.zero_grad()\n",
        "        D_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        ### Train Generator ###\n",
        "        D_fake_for_G = discriminator(input_image, fake_image)\n",
        "        G_GAN_loss = criterion_GAN(D_fake_for_G, torch.ones_like(D_fake_for_G).cuda())\n",
        "        G_L1_loss = criterion_L1(fake_image, target_image) * lambda_l1\n",
        "        G_loss = G_GAN_loss + G_L1_loss\n",
        "        optimizer_G.zero_grad()\n",
        "        G_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Update epoch loss\n",
        "        epoch_D_loss += D_loss.item()\n",
        "        epoch_G_loss += G_loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_description(f\"Epoch [{epoch}/{num_epochs}]\")\n",
        "        loop.set_postfix(D_loss=f\"{D_loss.item():.4f}\", G_loss=f\"{G_loss.item():.4f}\")\n",
        "    print(f\"Generator Loss is:{epoch_G_loss / len(train_loader)}\")\n",
        "    print(f\"Discriminator Loss is:{epoch_D_loss / len(train_loader)}\")\n",
        "\n",
        "    # Average losses for the epoch\n",
        "    generator_losses.append(epoch_G_loss / len(train_loader))\n",
        "    discriminator_losses.append(epoch_D_loss / len(train_loader))\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch,\n",
        "        \"generator_state_dict\": generator.state_dict(),\n",
        "        \"discriminator_state_dict\": discriminator.state_dict(),\n",
        "        \"optimizer_G_state_dict\": optimizer_G.state_dict(),\n",
        "        \"optimizer_D_state_dict\": optimizer_D.state_dict(),\n",
        "        \"generator_losses\": generator_losses,\n",
        "        \"discriminator_losses\": discriminator_losses,\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved for epoch {epoch}.\")\n",
        "\n",
        "    if (epoch) % 5 == 0:\n",
        "      with torch.no_grad():\n",
        "          generator.eval()\n",
        "\n",
        "          # Generate fake sample\n",
        "          input_image, target_image = next(iter(val_loader))\n",
        "          input_image, target_image = input_image.cuda(), target_image.cuda()\n",
        "          fake_sample = generator(input_image)\n",
        "\n",
        "          # Convert tensors to CPU and detach them for Matplotlib\n",
        "          input_img = input_image.squeeze(0).cpu().permute(1, 2, 0)  # Remove batch dimension and permute\n",
        "          target_img = target_image.squeeze(0).cpu().permute(1, 2, 0)  # Remove batch dimension and permute\n",
        "          generated_img = fake_sample.squeeze(0).detach().cpu().permute(1, 2, 0)  # Remove batch dimension and permute\n",
        "\n",
        "          # Plot images\n",
        "          plt.figure(figsize=(12, 4))\n",
        "\n",
        "          plt.subplot(1, 3, 1)\n",
        "          plt.imshow(input_img, cmap=\"gray\" if input_img.shape[-1] == 1 else None)\n",
        "          plt.title(\"Input Image\")\n",
        "          plt.axis(\"off\")\n",
        "\n",
        "          plt.subplot(1, 3, 2)\n",
        "          plt.imshow(target_img, cmap=\"gray\" if target_img.shape[-1] == 1 else None)\n",
        "          plt.title(\"Target Image\")\n",
        "          plt.axis(\"off\")\n",
        "\n",
        "          plt.subplot(1, 3, 3)\n",
        "          plt.imshow(generated_img, cmap=\"gray\" if generated_img.shape[-1] == 1 else None)\n",
        "          plt.title(\"Generated Image\")\n",
        "          plt.axis(\"off\")\n",
        "\n",
        "          # Show the plot\n",
        "          plt.show()\n",
        "    print(\"################################################################\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:50:30.245141Z",
          "iopub.execute_input": "2024-12-12T17:50:30.245505Z",
          "iopub.status.idle": "2024-12-12T17:53:31.980003Z",
          "shell.execute_reply.started": "2024-12-12T17:50:30.245472Z",
          "shell.execute_reply": "2024-12-12T17:53:31.978748Z"
        },
        "id": "GdIT1Nm9DKzk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Generator and Discriminator losses\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot Generator Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(0, 2), generator_losses, label=\"Generator Loss\", color='blue')\n",
        "plt.title(\"Generator Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot Discriminator Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(0, 2), discriminator_losses, label=\"Discriminator Loss\", color='red')\n",
        "plt.title(\"Discriminator Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Show plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:53:58.918693Z",
          "iopub.execute_input": "2024-12-12T17:53:58.919619Z",
          "iopub.status.idle": "2024-12-12T17:53:59.540410Z",
          "shell.execute_reply.started": "2024-12-12T17:53:58.919582Z",
          "shell.execute_reply": "2024-12-12T17:53:59.539595Z"
        },
        "id": "JVtYAKiPDKzl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Ensure the generator is in evaluation mode\n",
        "generator.eval()\n",
        "\n",
        "# Number of samples to display\n",
        "num_samples = 10\n",
        "\n",
        "# Lists to hold images\n",
        "input_images_list = []\n",
        "target_images_list = []\n",
        "generated_images_list = []\n",
        "\n",
        "# Collect images from the validation loader\n",
        "for i, (input_image, target_image) in enumerate(val_loader):\n",
        "    if len(input_images_list) >= num_samples:\n",
        "        break\n",
        "    input_images_list.append(input_image.squeeze(0).cuda())  # Remove batch dimension\n",
        "    target_images_list.append(target_image.squeeze(0).cuda())  # Remove batch dimension\n",
        "    with torch.no_grad():\n",
        "        generated_images_list.append(generator(input_image.cuda()).squeeze(0))  # Remove batch dimension\n",
        "\n",
        "# Plot the images\n",
        "fig, axes = plt.subplots(num_samples, 3, figsize=(12, num_samples * 3))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    # Input image\n",
        "    input_img = input_images_list[i].cpu().permute(1, 2, 0).detach()  # Convert to HWC\n",
        "    # Target image\n",
        "    target_img = target_images_list[i].cpu().permute(1, 2, 0).detach()  # Convert to HWC\n",
        "    # Generated image\n",
        "    generated_img = generated_images_list[i].cpu().permute(1, 2, 0).detach()  # Convert to HWC\n",
        "\n",
        "    # Plot the input image\n",
        "    axes[i, 0].imshow(input_img, cmap=\"gray\" if input_img.shape[-1] == 1 else None)\n",
        "    axes[i, 0].axis(\"off\")\n",
        "    if i == 0:\n",
        "        axes[i, 0].set_title(\"Input Image\")\n",
        "\n",
        "    # Plot the target image\n",
        "    axes[i, 1].imshow(target_img, cmap=\"gray\" if target_img.shape[-1] == 1 else None)\n",
        "    axes[i, 1].axis(\"off\")\n",
        "    if i == 0:\n",
        "        axes[i, 1].set_title(\"Target Image\")\n",
        "\n",
        "    # Plot the generated image\n",
        "    axes[i, 2].imshow(generated_img, cmap=\"gray\" if generated_img.shape[-1] == 1 else None)\n",
        "    axes[i, 2].axis(\"off\")\n",
        "    if i == 0:\n",
        "        axes[i, 2].set_title(\"Generated Image\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:54:09.439154Z",
          "iopub.execute_input": "2024-12-12T17:54:09.439536Z",
          "iopub.status.idle": "2024-12-12T17:54:12.003228Z",
          "shell.execute_reply.started": "2024-12-12T17:54:09.439504Z",
          "shell.execute_reply": "2024-12-12T17:54:12.001894Z"
        },
        "id": "suBdVTn6DKzl"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}